{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CNN Using Scratch And Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download the dataset from the below url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries as shown below\n",
    "\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-size all the images to this\n",
    "IMAGE_SIZE = [224, 224]\n",
    "\n",
    "train_path = 'cell_images/Train'\n",
    "valid_path = 'cell_images/Test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 793s 10us/step\n"
     ]
    }
   ],
   "source": [
    "# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG\n",
    "# Here we will be using imagenet weights\n",
    "\n",
    "mobilnet = VGG19(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't train existing weights\n",
    "for layer in mobilnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # useful for getting number of output classes\n",
    "folders = glob('Dataset/Train/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset/Train\\\\Parasite', 'Dataset/Train\\\\Uninfected']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our layers - you can add more if you want\n",
    "x = Flatten()(mobilnet.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(len(folders), activation='softmax')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=mobilnet.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 50178     \n",
      "=================================================================\n",
      "Total params: 20,074,562\n",
      "Trainable params: 50,178\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# view the structure of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Model from scratch using CNN\n",
    "model=Sequential()\n",
    "model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,activation=\"relu\"))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the model what cost and optimization method to use\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Image Data Generator to import the images from the dataset\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 416 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Make sure you provide the same target size as initialied for the image size\n",
    "training_set = train_datagen.flow_from_directory('Dataset/Train',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.preprocessing.image.DirectoryIterator at 0x22bb4c284e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('Dataset/Test',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kulde\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 136s 10s/step - loss: 1.5548 - accuracy: 0.4965 - val_loss: 0.5511 - val_accuracy: 0.7015\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 127s 10s/step - loss: 0.4928 - accuracy: 0.7569 - val_loss: 0.5099 - val_accuracy: 0.7015\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 133s 10s/step - loss: 0.4039 - accuracy: 0.8401 - val_loss: 0.4541 - val_accuracy: 0.7537\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 136s 11s/step - loss: 0.3441 - accuracy: 0.8359 - val_loss: 0.4364 - val_accuracy: 0.7612\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 131s 10s/step - loss: 0.2988 - accuracy: 0.8752 - val_loss: 0.3904 - val_accuracy: 0.8433\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 86s 6s/step - loss: 0.2806 - accuracy: 0.9051 - val_loss: 0.4214 - val_accuracy: 0.7836\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 79s 6s/step - loss: 0.2686 - accuracy: 0.9127 - val_loss: 0.3357 - val_accuracy: 0.8582\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.2747 - accuracy: 0.8878 - val_loss: 0.3230 - val_accuracy: 0.8731\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.2669 - accuracy: 0.9016 - val_loss: 0.3206 - val_accuracy: 0.8955\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 78s 6s/step - loss: 0.2404 - accuracy: 0.9068 - val_loss: 0.5069 - val_accuracy: 0.7239\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.3230 - accuracy: 0.8416 - val_loss: 0.3441 - val_accuracy: 0.7836\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 78s 6s/step - loss: 0.2644 - accuracy: 0.9039 - val_loss: 0.2914 - val_accuracy: 0.9104\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 78s 6s/step - loss: 0.1954 - accuracy: 0.9349 - val_loss: 0.3837 - val_accuracy: 0.8134\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1954 - accuracy: 0.9301 - val_loss: 0.3042 - val_accuracy: 0.8881\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1731 - accuracy: 0.9597 - val_loss: 0.2690 - val_accuracy: 0.8955\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1536 - accuracy: 0.9441 - val_loss: 0.2825 - val_accuracy: 0.8955\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 78s 6s/step - loss: 0.1237 - accuracy: 0.9626 - val_loss: 0.2792 - val_accuracy: 0.9179\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1410 - accuracy: 0.9610 - val_loss: 0.2527 - val_accuracy: 0.9254\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1224 - accuracy: 0.9820 - val_loss: 0.2441 - val_accuracy: 0.9328\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1491 - accuracy: 0.9684 - val_loss: 0.4121 - val_accuracy: 0.7612\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1414 - accuracy: 0.9458 - val_loss: 0.2383 - val_accuracy: 0.9254\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1184 - accuracy: 0.9753 - val_loss: 0.2522 - val_accuracy: 0.9179\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1268 - accuracy: 0.9624 - val_loss: 0.5683 - val_accuracy: 0.7313\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1895 - accuracy: 0.9140 - val_loss: 0.2040 - val_accuracy: 0.9478\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1443 - accuracy: 0.9504 - val_loss: 0.2215 - val_accuracy: 0.9254\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1018 - accuracy: 0.9696 - val_loss: 0.2686 - val_accuracy: 0.8806\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1122 - accuracy: 0.9610 - val_loss: 0.2523 - val_accuracy: 0.9030\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1125 - accuracy: 0.9609 - val_loss: 0.2382 - val_accuracy: 0.9104\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 78s 6s/step - loss: 0.1152 - accuracy: 0.9744 - val_loss: 0.3537 - val_accuracy: 0.8284\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1561 - accuracy: 0.9407 - val_loss: 0.2002 - val_accuracy: 0.9254\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1191 - accuracy: 0.9792 - val_loss: 0.2510 - val_accuracy: 0.8657\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1249 - accuracy: 0.9473 - val_loss: 0.3240 - val_accuracy: 0.8582\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1684 - accuracy: 0.9334 - val_loss: 0.1891 - val_accuracy: 0.9403\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1065 - accuracy: 0.9638 - val_loss: 0.2600 - val_accuracy: 0.9104\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1128 - accuracy: 0.9633 - val_loss: 0.1841 - val_accuracy: 0.9478\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1256 - accuracy: 0.9582 - val_loss: 0.1991 - val_accuracy: 0.9254\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.1267 - accuracy: 0.9563 - val_loss: 0.2524 - val_accuracy: 0.8881\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.0986 - accuracy: 0.9667 - val_loss: 0.2134 - val_accuracy: 0.9104\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.0890 - accuracy: 0.9700 - val_loss: 0.2278 - val_accuracy: 0.9030\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.0707 - accuracy: 0.9863 - val_loss: 0.1967 - val_accuracy: 0.9179\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.0775 - accuracy: 0.9702 - val_loss: 0.2413 - val_accuracy: 0.9104\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 78s 6s/step - loss: 0.0732 - accuracy: 0.9827 - val_loss: 0.1775 - val_accuracy: 0.9328\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 77s 6s/step - loss: 0.0788 - accuracy: 0.9747 - val_loss: 0.2521 - val_accuracy: 0.8881\n",
      "Epoch 44/50\n",
      "10/13 [======================>.......] - ETA: 29s - loss: 0.0965 - accuracy: 0.9793"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "# Run the cell. It will take some time to execute\n",
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=50,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(r.history['loss'], label='train loss')\n",
    "plt.plot(r.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('LossVal_loss')\n",
    "\n",
    "# plot the accuracy\n",
    "plt.plot(r.history['accuracy'], label='train acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('AccVal_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as a h5 file\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.save('model_vgg19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('model_vgg19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=image.load_img('Dataset/Test/Uninfected/2.png',target_size=(224,224))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=image.img_to_array(img)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.expand_dims(x,axis=0)\n",
    "img_data=preprocess_input(x)\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.argmax(model.predict(img_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(a==1):\n",
    "    print(\"Uninfected\")\n",
    "else:\n",
    "    print(\"Infected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
